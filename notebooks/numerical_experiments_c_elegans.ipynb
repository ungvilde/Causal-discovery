{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2818ca-da31-4771-81fc-fa95b89ee854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import statsmodels.api as sm\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from spikeometric.models import BernoulliGLM\n",
    "from spikeometric.datasets import NormalGenerator, ConnectivityDataset\n",
    "from spikeometric.stimulus import RegularStimulus\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, from_networkx\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from CD_methods import SCM_learner\n",
    "from functions import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516d4dda-3f6d-4f4a-a971-e70e95ea6de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "network_data = torch.load('../data/c_elegans_data.pt')\n",
    "\n",
    "with open('../data/c_elegans_spike_data_single_node_stimuli.pickle', 'rb') as f:\n",
    "    spike_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54b1b9a0-4e81-4ed3-93f7-6daa9e6eb742",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '7', '10', '37', '68', '76', '112', '119', '132', '207', '231', '237', '238', '251', '268', 'null']\n",
      "num experiments= 16\n"
     ]
    }
   ],
   "source": [
    "n_neurons = network_data.num_nodes\n",
    "\n",
    "G = to_networkx(network_data, node_attrs = ['position'])\n",
    "position_dict = nx.get_node_attributes(G, 'position')\n",
    "\n",
    "# sample neurons\n",
    "n_obs = 50\n",
    "index_obs = np.sort(np.random.choice(n_neurons, size = n_obs, replace = False))\n",
    "\n",
    "# design stimulation protocol\n",
    "#stimulation_protocol = [[i] for i in index_obs]\n",
    "prop_intervened = 0.3\n",
    "stimulate_nodes = np.sort(np.random.choice(index_obs, size = int(n_obs*prop_intervened), replace = False))\n",
    "stimulation_protocol = [[i] for i in stimulate_nodes]\n",
    "stimulation_protocol_str = [str(i) for i in stimulate_nodes] + ['null']\n",
    "print(stimulation_protocol_str)\n",
    "print('num experiments=', len(stimulation_protocol_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15a9a39a-1418-43b8-a809-cd336b651096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spike_data_obs = dict()\n",
    "spike_data_obs['null'] = spike_data['null'][index_obs]\n",
    "for intervention in index_obs:\n",
    "    spike_data_obs[str(intervention)] = spike_data[str(intervention)][index_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9ea5dc9-3e3f-4f14-8eeb-ddc11f31f318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num. confounding variables =  92\n"
     ]
    }
   ],
   "source": [
    "G_obs = nx.subgraph(G, index_obs)\n",
    "index_hidden = [node for node in range(n_neurons) if node not in index_obs]\n",
    "confounders = []\n",
    "for node in index_hidden:\n",
    "    count = 0\n",
    "    for _, v in G.out_edges(node):\n",
    "        if v in index_obs:\n",
    "            count += 1\n",
    "    if count >= 2:\n",
    "        confounders.append(node)\n",
    "print('num. confounding variables = ',len(confounders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32eb2466-eb81-4167-9e14-768bd8ac24a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 15/15 [00:00<00:00, 314.11it/s]\n"
     ]
    }
   ],
   "source": [
    "G_learned = SCM_learner(spike_data_obs, \n",
    "                        node_list=index_obs, \n",
    "                        stimulation_protocol=stimulation_protocol, \n",
    "                        alpha = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b552b65-01eb-4ccb-8482-2da4e523de1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num. confounding variables =  92\n",
      "total edges (in true observed graph) =  63\n",
      "percentage of nodes observed =  17.92 %\n",
      "SHD = 15\n",
      "sensitivity= 0.8077\n",
      "specificity= 1.0\n"
     ]
    }
   ],
   "source": [
    "G_true = nx.subgraph(G, index_obs)\n",
    "\n",
    "A_true = nx.adjacency_matrix(G_true, nodelist=index_obs).todense() \n",
    "A_learned = nx.adjacency_matrix(G_learned, nodelist=index_obs).todense() \n",
    "A_diff = A_true - A_learned\n",
    "\n",
    "SHD = np.sum(np.abs(A_true- A_learned))\n",
    "TP = np.sum( (A_true == 1)*(A_learned==1) )\n",
    "TN = np.sum( (A_true == 0)*(A_learned==0)) \n",
    "FP = np.sum(A_diff == -1)\n",
    "FN = np.sum(A_diff == 1) \n",
    "\n",
    "index_hidden = [node for node in range(n_neurons) if node not in index_obs]\n",
    "confounders = []\n",
    "for node in index_hidden:\n",
    "    count = 0\n",
    "    for _, v in G.out_edges(node):\n",
    "        if v in index_obs:\n",
    "            count += 1\n",
    "    if count >= 2:\n",
    "        confounders.append(node)\n",
    "print('num. confounding variables = ', len(confounders))\n",
    "\n",
    "print('total edges (in true observed graph) = ',G_true.number_of_edges())\n",
    "print('percentage of nodes observed = ', np.round(G_obs.number_of_nodes() / G.number_of_nodes() * 100, 2), '%')\n",
    "print('SHD =',compute_SHD(G_true, G_learned))\n",
    "print('sensitivity=', np.round(compute_sensitivity(G_true, G_learned, nodelist=index_obs), 4))\n",
    "print('specificity=', np.round(compute_specificity(G_true, G_learned, nodelist=index_obs), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33e9bbaa-c38e-4d6f-b022-7207a8f5a37f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_obs =  80 proportion intervened =  0.1\n",
      "['18', '90', '110', '136', '168', '169', '177', '231', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 81.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 63\n",
      "sensitivity= 0.7225\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  0.2\n",
      "['33', '71', '103', '106', '127', '147', '148', '161', '169', '170', '171', '192', '195', '215', '234', '240', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 67.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 62\n",
      "sensitivity= 0.7033\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  0.30000000000000004\n",
      "['35', '50', '55', '67', '84', '88', '92', '100', '103', '104', '110', '125', '128', '131', '137', '146', '151', '189', '197', '222', '232', '233', '249', '268', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 24/24 [00:00<00:00, 78.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 58\n",
      "sensitivity= 0.7883\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  0.4\n",
      "['4', '10', '21', '29', '31', '41', '44', '54', '60', '71', '94', '95', '102', '114', '115', '117', '120', '125', '135', '139', '148', '159', '178', '193', '221', '222', '226', '228', '232', '250', '272', '273', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 66.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 48\n",
      "sensitivity= 0.7714\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  0.5\n",
      "['0', '3', '4', '7', '8', '18', '28', '29', '35', '47', '53', '57', '74', '77', '87', '93', '95', '107', '108', '113', '119', '122', '127', '128', '133', '135', '159', '167', '176', '178', '183', '187', '203', '207', '227', '229', '230', '250', '261', '264', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:00<00:00, 65.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 40\n",
      "sensitivity= 0.8444\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  0.6000000000000001\n",
      "['7', '15', '29', '31', '33', '38', '39', '41', '42', '43', '47', '59', '71', '79', '89', '91', '93', '97', '99', '113', '118', '119', '128', '130', '131', '133', '138', '143', '146', '162', '185', '194', '197', '198', '204', '210', '211', '215', '221', '224', '232', '247', '251', '255', '268', '269', '270', '277', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 91.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 21\n",
      "sensitivity= 0.8618\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  0.7000000000000001\n",
      "['1', '2', '4', '5', '8', '11', '13', '18', '26', '29', '33', '44', '58', '59', '64', '68', '73', '84', '96', '101', '102', '105', '108', '114', '122', '124', '137', '143', '151', '153', '154', '156', '157', '166', '169', '171', '173', '174', '175', '179', '180', '184', '191', '204', '220', '221', '233', '236', '240', '246', '250', '253', '258', '261', '271', '273', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 56/56 [00:00<00:00, 90.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 24\n",
      "sensitivity= 0.8509\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  0.8\n",
      "['4', '13', '14', '18', '19', '20', '24', '29', '31', '38', '41', '42', '46', '50', '54', '58', '67', '79', '83', '84', '87', '101', '106', '111', '113', '118', '120', '121', '128', '132', '137', '139', '147', '149', '162', '164', '165', '172', '179', '180', '181', '183', '186', '189', '190', '197', '204', '209', '215', '221', '227', '230', '234', '236', '237', '239', '241', '244', '249', '254', '257', '266', '273', '278', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 64/64 [00:01<00:00, 46.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 21\n",
      "sensitivity= 0.8934\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  0.9\n",
      "['3', '7', '10', '12', '19', '32', '42', '50', '55', '56', '57', '59', '63', '68', '69', '70', '72', '80', '85', '99', '100', '101', '107', '108', '111', '118', '121', '131', '141', '142', '143', '144', '145', '148', '149', '150', '156', '157', '163', '166', '170', '172', '173', '176', '178', '180', '181', '183', '187', '191', '196', '201', '210', '213', '214', '224', '225', '229', '232', '233', '237', '241', '242', '245', '249', '252', '255', '263', '264', '267', '268', '272', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 72/72 [00:01<00:00, 58.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 7\n",
      "sensitivity= 0.9662\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80 proportion intervened =  1.0\n",
      "['1', '2', '13', '17', '21', '22', '24', '29', '38', '41', '54', '55', '58', '60', '63', '64', '66', '67', '68', '72', '75', '77', '79', '83', '86', '87', '89', '100', '102', '112', '118', '120', '121', '122', '125', '126', '128', '131', '133', '136', '137', '138', '142', '154', '158', '159', '160', '161', '166', '168', '169', '170', '174', '179', '181', '194', '198', '204', '212', '213', '217', '219', '222', '226', '227', '229', '231', '238', '241', '243', '246', '249', '251', '257', '261', '263', '264', '267', '268', '277', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:01<00:00, 75.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 1\n",
      "sensitivity= 0.994\n",
      "specificity= 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_random_single_node_intervention = []\n",
    "\n",
    "#for n_obs in np.arange(10, 100, 10):\n",
    "n_obs = 80\n",
    "for p in np.arange(0.1, 1.01, 0.1):\n",
    "\n",
    "    print('n_obs = ', n_obs, 'proportion intervened = ', np.round(p, 2))\n",
    "    index_obs = np.sort(np.random.choice(n_neurons, size = n_obs, replace = False))\n",
    "\n",
    "    # select intervened nodes randomly\n",
    "    stimulate_nodes = np.sort(np.random.choice(index_obs, size = int(n_obs*p), replace = False))\n",
    "    stimulation_protocol = [[i] for i in stimulate_nodes]\n",
    "    stimulation_protocol_str = [str(i) for i in stimulate_nodes] + ['null']\n",
    "\n",
    "    # get data\n",
    "    spike_data_obs = dict()\n",
    "    spike_data_obs['null'] = spike_data['null'][index_obs]\n",
    "    for intervention in index_obs:\n",
    "        spike_data_obs[str(intervention)] = spike_data[str(intervention)][index_obs]\n",
    "\n",
    "\n",
    "    G_learned = SCM_learner(spike_data_obs, \n",
    "                        node_list=index_obs, \n",
    "                        stimulation_protocol=stimulation_protocol, \n",
    "                        alpha = 0.01)\n",
    "\n",
    "    G_true = nx.subgraph(G, index_obs)\n",
    "\n",
    "    print('SHD =',compute_SHD(G_true, G_learned))\n",
    "    print('sensitivity=', np.round(compute_sensitivity(G_true, G_learned, nodelist=index_obs), 4))\n",
    "    print('specificity=', np.round(compute_specificity(G_true, G_learned, nodelist=index_obs), 4))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f2fc5-9d89-4f07-8eb2-62d559428586",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Seems to be a 'critical threshold' for causal discovery, where the learned graph is reliable and accurate whenever we observe more than 28 % of the network \n",
    " - Why this happens is unclear. But somehow the signal of the observed neurons is not strong enough with small samples. \n",
    "     Could it be:\n",
    "     - Some set of 'critical nodes' are almost always included in the sampled data, fx high degree nodes\n",
    "     - The number of confounders relative to observed nodes is reduced\n",
    "- Need to investigate: \n",
    "    - The exact number of nodes required for threshold to be reached\n",
    "    - Maybe look at the out- and in-degree distribution of nodes observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd51cb03-5428-4332-a06e-45144ebaa774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "sample_space = list(np.arange(279))\n",
    "sample_space.remove(0)\n",
    "sample_space.remove(6)\n",
    "sample_space.remove(12)\n",
    "sample_space.remove(98)\n",
    "sample_space.remove(111)\n",
    "sample_space.remove(129)\n",
    "sample_space.remove(134)\n",
    "sample_space.remove(142)\n",
    "sample_space.remove(230)\n",
    "sample_space.remove(238)\n",
    "sample_space.remove(143)\n",
    "sample_space.remove(146)\n",
    "sample_space.remove(188)\n",
    "sample_space.remove(270)\n",
    "\n",
    "print(len(sample_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51f29a37-baab-493e-a7f5-76e0224257a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_obs =  72\n",
      "num. confounding variables =  95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 72/72 [00:00<00:00, 108.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 0\n",
      "sensitivity= 1.0\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  74\n",
      "num. confounding variables =  104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 74/74 [00:00<00:00, 81.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 0\n",
      "sensitivity= 1.0\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  76\n",
      "num. confounding variables =  112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 76/76 [00:00<00:00, 109.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 0\n",
      "sensitivity= 1.0\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  78\n",
      "num. confounding variables =  94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 78/78 [00:00<00:00, 104.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 1\n",
      "sensitivity= 0.9928\n",
      "specificity= 1.0\n",
      "\n",
      "n_obs =  80\n",
      "num. confounding variables =  104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:00<00:00, 91.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHD = 0\n",
      "sensitivity= 1.0\n",
      "specificity= 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n_obs in np.arange(70, 80, 2):\n",
    "    index_obs = list(np.random.choice(128, size = n_obs, replace = False))\n",
    "    index_obs += [146, 188]\n",
    "    index_obs=np.sort(index_obs)\n",
    "    print('n_obs = ', len(index_obs))\n",
    "    \n",
    "    stimulation_protocol = [[i] for i in index_obs]\n",
    "    spike_data_obs = dict()\n",
    "    spike_data_obs['null'] = spike_data['null'][index_obs]\n",
    "    for intervention in index_obs:\n",
    "        spike_data_obs[str(intervention)] = spike_data[str(intervention)][index_obs]\n",
    "    \n",
    "    # count num. confounders\n",
    "    G_obs = nx.subgraph(G, index_obs)\n",
    "    index_hidden = [node for node in range(n_neurons) if node not in index_obs]\n",
    "    confounders = []\n",
    "    for node in index_hidden:\n",
    "        count = 0\n",
    "        for _, v in G.out_edges(node):\n",
    "            if v in index_obs:\n",
    "                count += 1\n",
    "        if count >= 2:\n",
    "            confounders.append(node)\n",
    "    print('num. confounding variables = ',len(confounders))\n",
    "    \n",
    "    G_learned = SCM_learner(spike_data_obs, \n",
    "                        node_list=index_obs, \n",
    "                        stimulation_protocol=stimulation_protocol, \n",
    "                        alpha = 0.01)\n",
    "    \n",
    "    G_true = nx.subgraph(G, index_obs)\n",
    "\n",
    "    print('SHD =',compute_SHD(G_true, G_learned))\n",
    "    print('sensitivity=', np.round(compute_sensitivity(G_true, G_learned, nodelist=index_obs), 4))\n",
    "    print('specificity=', np.round(compute_specificity(G_true, G_learned, nodelist=index_obs), 4))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab58242-f90e-4297-828b-bc4bee0b08bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notes\n",
    "- when high degree nodes are included in dataset, the learner struggles below 28% threshold. if not included, it still works. might be other 'critical nodes' to consider, like 0 and 6. Node 129 seems to be a problem.\n",
    "- removing all nodes with in_degree > 20 from observed data is also bad...\n",
    "- This info can be useful wrt designing experiments! see how many experiments are needed to get good results when targeting high degree nodes first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275cdf7-fb63-4bdd-a789-9224b1858eb6",
   "metadata": {},
   "source": [
    "### Update\n",
    "- I was making a mistake when computing SHD, turns out there is not a threshold????? Weird, because it seemed like some nodes where making a big difference yesterday\n",
    "- Now complete single node stimulation protocol will give a basically perfect result for any size of the observed network. I guess that is in line with what we should expect, given the fact that the model captures every feature of the spiking pattern and we correct for any confounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e666a-ba8e-4290-bd64-57744a2039c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikeenv",
   "language": "python",
   "name": "spikeenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
